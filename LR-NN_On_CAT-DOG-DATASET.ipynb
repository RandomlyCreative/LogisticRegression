{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nimport random\nimport cv2\nimport os\nfrom math import ceil\n\nROWS = 64\nCOLS = 64\nCHANNELS=3","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_dir_cats = \"../input/cat-and-dog/training_set/training_set/cats/\"\ntest_dir_cats = \"../input/cat-and-dog/test_set/test_set/cats/\"\n\ntrain_images_cats=[train_dir_cats+i for i in os.listdir(train_dir_cats)]\ntest_images_cats=[test_dir_cats+i for i in os.listdir(test_dir_cats)]\n\ntrain_dir_dogs = \"../input/cat-and-dog/training_set/training_set/dogs/\"\ntest_dir_dogs = \"../input/cat-and-dog/test_set/test_set/dogs/\"\n\ntrain_images_dogs=[train_dir_dogs+i for i in os.listdir(train_dir_dogs)]\ntest_images_dogs=[test_dir_dogs+i for i in os.listdir(test_dir_dogs)]\n\ntrain_images = train_images_cats + train_images_dogs\ntest_images = test_images_cats + test_images_dogs\n\ntrain_images.remove('../input/cat-and-dog/training_set/training_set/dogs/_DS_Store')\ntrain_images.remove('../input/cat-and-dog/training_set/training_set/cats/_DS_Store')\ntest_images.remove('../input/cat-and-dog/test_set/test_set/dogs/_DS_Store')\ntest_images.remove('../input/cat-and-dog/test_set/test_set/cats/_DS_Store')\nrandom.shuffle(train_images)\nrandom.shuffle(test_images)\n","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_images))\nprint(len(test_images))","execution_count":55,"outputs":[{"output_type":"stream","text":"8005\n2023\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"tn=[]\ntt=[]\nfor i in range(2000):\n    tn.append(train_images[i])\nfor j in range(100):\n    tt.append(test_images[j])\ntrain_images=tn\ntest_images=tt\nprint(len(train_images))\nprint(len(test_images))\"\"\"","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"'tn=[]\\ntt=[]\\nfor i in range(2000):\\n    tn.append(train_images[i])\\nfor j in range(100):\\n    tt.append(test_images[j])\\ntrain_images=tn\\ntest_images=tt\\nprint(len(train_images))\\nprint(len(test_images))'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(file_path):\n    if file_path.endswith('.jpg'):\n        img = cv2.imread(file_path, 1)\n    return cv2.resize(img, (ROWS,COLS), interpolation=cv2.INTER_CUBIC)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(images):\n\n    m = len(images)\n    X = np.zeros((m, ROWS, COLS, CHANNELS),dtype=np.uint8)\n    y = np.zeros((1, m))\n    for i, image_file in enumerate(images):\n        #print(image_file)\n        X[i,:] = read_image(image_file)\n        if 'dog.' in image_file.lower():\n            #print(\"dog\")\n            y[0, i] = 1\n        elif 'cat.' in image_file.lower():\n            y[0, i] = 0\n            #print(\"cat\")\n    return X, y","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_x, train_set_y = prepare_data(train_images)\ntest_set_x, test_set_y = prepare_data(test_images)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_x_flatten = train_set_x.reshape(train_set_x.shape[0], CHANNELS*COLS*ROWS).T\ntest_set_x_flatten = test_set_x.reshape(test_set_x.shape[0], -1).T\ntest_set_x=test_set_x_flatten/255\ntrain_set_x=train_set_x_flatten/255\n","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    s = 1/(1+np.exp(-z))\n    return s","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n\n    w = np.zeros((dim,1))\n    b = 0\n\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    A = sigmoid(np.dot(w.T, X)+ b)\n    \n    cost = (-1.0) * np.mean(np.multiply(Y, np.log(A)) + np.multiply(1.0-Y, np.log(1.0 - A)), axis=1)                                 # compute cost\n    \n    dw = (1/m)*np.dot(X,np.transpose(A-Y))\n    db = np.average((A-Y))\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = np.array([[1.],[2.]])\nb = 4.\nX = np.array([[5., 6., -7.],[8., 9., -10.]])\nY = np.array([[1,0,1]])\n\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))","execution_count":72,"outputs":[{"output_type":"stream","text":"dw = [[4.33333333]\n [6.33333333]]\ndb = 2.934645119504845e-11\ncost = 16.999996678946573\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        \n        \n        if i % 100 == 0:\n            costs.append(cost)\n        \n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params, grads, costs = optimize(w, b, X, Y, num_iterations = 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))","execution_count":74,"outputs":[{"output_type":"stream","text":"w = [[-0.49157334]\n [-0.16017651]]\nb = 3.948381664135624\ndw = [[ 0.03602232]\n [-0.02064108]]\ndb = -0.01897084202791009\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T,X)+b)\n    \n    for i in range(A.shape[1]):\n        \n        if A[0,i] > 0.5:\n            Y_prediction[0,i] = 1\n        else:\n            Y_prediction[0,i] = 0\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.003, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)\n    \n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.003, print_cost = True)","execution_count":67,"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.693147\nCost after iteration 100: 0.672402\nCost after iteration 200: 0.665198\nCost after iteration 300: 0.660495\nCost after iteration 400: 0.656892\nCost after iteration 500: 0.653895\nCost after iteration 600: 0.651276\nCost after iteration 700: 0.648911\nCost after iteration 800: 0.646730\nCost after iteration 900: 0.644689\nCost after iteration 1000: 0.642759\nCost after iteration 1100: 0.640922\nCost after iteration 1200: 0.639163\nCost after iteration 1300: 0.637474\nCost after iteration 1400: 0.635846\nCost after iteration 1500: 0.634274\nCost after iteration 1600: 0.632752\nCost after iteration 1700: 0.631278\nCost after iteration 1800: 0.629847\nCost after iteration 1900: 0.628457\ntrain accuracy: 65.37164272329794 %\ntest accuracy: 60.1581809194266 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}